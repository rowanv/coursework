{
 "metadata": {
  "name": "",
  "signature": "sha256:b36514145e413551d895027257a8d42721131baeda8d38b54b07a60f755e86a3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Basic Commands"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Graphics"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Indexing Data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Loading Data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Adding Additional Graphical and Numerical Summaries "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "2.4 Exercises"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. a. If the sample size n is extremely large, and the number of predictors p is small, we would generally expect a flexible statistical learning method to be better. The large sample size should allow for us to decrease the variance.\n",
      "\n",
      "b. If hte number of predictors p is exremely large and the number of observations n is small, we would generally expect the performance of an inflexible method to be better.\n",
      "\n",
      "c. If the relationship between the predictors and the response is highly non-linear, we wsould generally expect the performance of a flexible statistical learning method to be better, the more flexible it is, the more complex the polynomial that approximates the relationship between teh two. \n",
      "\n",
      "d. If the variance of the error terms is extremely high, we would expect for a flexible method to be worse, as it would fit to the noise in the data set. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2 - \n",
      "a. This is a regression problem, since our dependent variablye is numerical. We are interested in inference, since we want to understand the factors that affect our dependent variable, rather than predict our dependent variable.\n",
      "n: 500 firms\n",
      "p: profit, number of employees, industry, CEO salary.\n",
      "\n",
      "b. This is a classification problem. We are most interested in prediction, since we want to know whether our new observation will fall into the success or failure bucket. \n",
      "n: 20 products\n",
      "p: success or failure, price charged for product, marketing budget, competition price, 10 other variables. \n",
      "\n",
      "c. This is a regression, prediction problem. \n",
      "n: 52 weeks in 2012\n",
      "p: % change in the dollar, % change in the US market, % chang ein the British market, % change in the German market. \n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3 - Bias-variance decomposition\n",
      "\n",
      "a. - Draw an image - \n",
      "\n",
      "b. Bias squared curve: bias describes the degree to which we add error by training a model that simplifies real-life behaviour. Therefore, our bias is high for inflexible models, but decreases for more flexible models.\n",
      "\n",
      "Variance: variance describes the degree to which our model would change if we used a different data set -- how much our trained model differs from a model trained using the full population. Therefore, variance starts out low for less flexible models (since they pick up less noise) but increases for more flexible models. \n",
      "\n",
      "Mean squared error: This is equal to the sum of bias squared + variance, so we see a u-shaped curve. \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "4, 5 - "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "6 - Parametric approaches make underlying assumptions about the nature of the variable distributions. Non-parametric approaches do not. This makes non-parametric approaches more flexible. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}